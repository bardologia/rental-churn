{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe5ec65",
   "metadata": {},
   "source": [
    "# Hierarchical Model for Payment Default Prediction\n",
    "\n",
    "This notebook uses the **hierarchical architecture** that:\n",
    "1. Groups invoices by user\n",
    "2. Models the temporal trajectory of each user's payment behavior\n",
    "3. Uses LSTM + Attention to capture sequential patterns\n",
    "\n",
    "This is more appropriate for payment data than treating each invoice independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "from Model.core import run_pipeline\n",
    "from Model.data_sequential import SequentialDataModule\n",
    "from Model.network_hierarchical import HierarchicalModel\n",
    "from Model.trainer_sequential import SequentialTrainer\n",
    "from Configs.config import config\n",
    "from Utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "project_root = current_dir\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_hierarchical\"\n",
    "run_dir = os.path.join(project_root, \"runs\", run_id)\n",
    "checkpoint_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "model_save_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "\n",
    "raw_path = os.path.join(project_root, config.paths.raw_data)\n",
    "data_path = os.path.join(project_root, config.paths.train_data)\n",
    "\n",
    "# Regenerate data with new features\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "    run_pipeline(raw_path, data_path)\n",
    "    print(f\"Generated new training data: {data_path}\")\n",
    "\n",
    "logger = Logger(log_dir=run_dir)\n",
    "print(f\"Run directory: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61514e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "runs_dir = os.path.join(project_root, \"runs\")\n",
    "print(f\"Starting TensorBoard on {runs_dir}...\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"{runs_dir}\" --port 6015 --reload_interval 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd698506",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Sequential Data Module - groups by user\n",
    "    dm = SequentialDataModule(\n",
    "        data_path,\n",
    "        batch_size=32,  # Smaller batch for sequences\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        logger=logger,\n",
    "        max_seq_len=50,  # Max invoices per user sequence\n",
    "        min_seq_len=2    # Min invoices to include user\n",
    "    )\n",
    "    dm.prepare_data()\n",
    "    \n",
    "    # Hierarchical Model\n",
    "    model = HierarchicalModel(\n",
    "        embedding_dims=dm.emb_dims,\n",
    "        n_cont=len(dm.cont_cols),\n",
    "        hidden_dim=config.model.hidden_dim,\n",
    "        n_invoice_layers=1,      # Layers for encoding single invoice\n",
    "        n_sequence_layers=2,     # LSTM layers for temporal modeling\n",
    "        n_heads=config.model.n_heads,\n",
    "        dropout=config.model.dropout,\n",
    "        use_temporal_attention=True  # Attention over history\n",
    "    )\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Hierarchical Model Parameters: {n_params:,}\")\n",
    "    \n",
    "    # Sequential Trainer\n",
    "    trainer = SequentialTrainer(\n",
    "        model, dm,\n",
    "        epochs=config.model.epochs,\n",
    "        lr=config.model.lr,\n",
    "        weight_decay=config.model.weight_decay,\n",
    "        logger=logger,\n",
    "        patience=config.model.patience,\n",
    "        mixed_precision=config.model.mixed_precision,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        max_grad_norm=config.model.max_grad_norm,\n",
    "        scheduler_factor=config.model.scheduler_factor,\n",
    "        scheduler_patience=config.model.scheduler_patience,\n",
    "        min_lr=config.model.min_lr,\n",
    "        loss_type=config.model.loss_type,\n",
    "        focal_alpha=config.model.focal_alpha,\n",
    "        focal_gamma=config.model.focal_gamma\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    best_model = trainer.fit()\n",
    "    \n",
    "    # Test\n",
    "    test_metrics = trainer.test(dm.test_dataloader())\n",
    "    \n",
    "    # Save\n",
    "    torch.save(best_model.state_dict(), model_save_path)\n",
    "    print(f\"\\nTraining finished! Model saved to {model_save_path}\")\n",
    "    print(f\"Test AUC-ROC: {test_metrics['auc_roc']:.4f}\")\n",
    "    print(f\"Test AUC-PR: {test_metrics['auc_pr']:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    logger.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
